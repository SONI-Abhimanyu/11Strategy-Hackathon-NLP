{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf65f373",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf65f373",
    "outputId": "5670882c-5fb3-435a-c442-07f151cabaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\nevin\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nevin\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nevin\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nevin\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nevin\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nevin\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob, Word\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import collections\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#from stop_words_list import stop_words_list\n",
    "#from wn_affect import wn_affect \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad6d8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48ad6d8f",
    "outputId": "45006cb6-7be0-4295-d776-2d9a0e275085"
   },
   "outputs": [],
   "source": [
    "pip install WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856d9594",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "856d9594",
    "outputId": "22b2507f-e880-406e-d39a-77a9cf85680e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d63f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "411d63f9",
    "outputId": "b06f736b-1f12-422f-cbc1-5569970c92ed"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ec468f",
   "metadata": {
    "id": "e6ec468f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nevin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (2,3,5,10,12,13,15,16,17,18,19,20,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Merged_data_without_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbbd99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "badbbd99",
    "outputId": "11d10750-f8ba-408e-df96-02a41d26b36f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U_FUNRqq2VYq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_FUNRqq2VYq",
    "outputId": "1ffe82e9-c7f5-4fea-acc3-9313f682c82e"
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d0c9c",
   "metadata": {
    "id": "8e2d0c9c"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48826cd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48826cd0",
    "outputId": "9f260f0d-8098-4ee5-bb98-349bc715051e"
   },
   "outputs": [],
   "source": [
    "len(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217401d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a217401d",
    "outputId": "691e92b4-ec71-4151-d177-fb0ba6e6874c"
   },
   "outputs": [],
   "source": [
    "len(test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaefbc8",
   "metadata": {
    "id": "6aaefbc8"
   },
   "source": [
    "# DataPreprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16666a30",
   "metadata": {
    "id": "16666a30"
   },
   "source": [
    "First step is tokenization which converts the text into sentences and the sentences into words. Convert the words into lowercase and remove punctionation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e478cc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My flight from Dulles to Paris was great. My f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went for 2 months to Nice, when I arrived i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday Jan 25, we flew Colombo, Sri Lanka t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First medium-haul experience with Air France....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOD-CDG-ATL: Was having some major issue with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body\n",
       "0  My flight from Dulles to Paris was great. My f...\n",
       "1   I went for 2 months to Nice, when I arrived i...\n",
       "2  Yesterday Jan 25, we flew Colombo, Sri Lanka t...\n",
       "3   First medium-haul experience with Air France....\n",
       "4   BOD-CDG-ATL: Was having some major issue with..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['review_body']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb45645",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "aeb45645",
    "outputId": "f63ed8de-4bae-487b-a6d1-3252fe0e93a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-66199223f102>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['remove_lower_punct'] = data['review_body'].str.lower().str.replace(\"'\", '').str.replace('[^\\w\\s]', ' ').str.replace(\" \\d+\", \" \").str.replace(' +', ' ').str.strip()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>remove_lower_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My flight from Dulles to Paris was great. My f...</td>\n",
       "      <td>my flight from dulles to paris was great my fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went for 2 months to Nice, when I arrived i...</td>\n",
       "      <td>i went for months to nice when i arrived in ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday Jan 25, we flew Colombo, Sri Lanka t...</td>\n",
       "      <td>yesterday jan we flew colombo sri lanka to par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First medium-haul experience with Air France....</td>\n",
       "      <td>first medium haul experience with air france t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOD-CDG-ATL: Was having some major issue with...</td>\n",
       "      <td>bod cdg atl was having some major issue with d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  \\\n",
       "0  My flight from Dulles to Paris was great. My f...   \n",
       "1   I went for 2 months to Nice, when I arrived i...   \n",
       "2  Yesterday Jan 25, we flew Colombo, Sri Lanka t...   \n",
       "3   First medium-haul experience with Air France....   \n",
       "4   BOD-CDG-ATL: Was having some major issue with...   \n",
       "\n",
       "                                  remove_lower_punct  \n",
       "0  my flight from dulles to paris was great my fl...  \n",
       "1  i went for months to nice when i arrived in ni...  \n",
       "2  yesterday jan we flew colombo sri lanka to par...  \n",
       "3  first medium haul experience with air france t...  \n",
       "4  bod cdg atl was having some major issue with d...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#case text to lower case, remove punctuation, remove extra whitepsace in the string and on both sides of the string\n",
    "data['remove_lower_punct'] = data['review_body'].str.lower().str.replace(\"'\", '').str.replace('[^\\w\\s]', ' ').str.replace(\" \\d+\", \" \").str.replace(' +', ' ').str.strip()\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e05323",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36e05323",
    "outputId": "38461251-3b27-4822-9750-e3febcb484d5"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068d3bdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "068d3bdf",
    "outputId": "16e485ec-de32-4ece-ad48-ce5037975dc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>remove_lower_punct</th>\n",
       "      <th>tokenise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My flight from Dulles to Paris was great. My f...</td>\n",
       "      <td>my flight from dulles to paris was great my fl...</td>\n",
       "      <td>[my, flight, from, dulles, to, paris, was, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went for 2 months to Nice, when I arrived i...</td>\n",
       "      <td>i went for months to nice when i arrived in ni...</td>\n",
       "      <td>[i, went, for, months, to, nice, when, i, arri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday Jan 25, we flew Colombo, Sri Lanka t...</td>\n",
       "      <td>yesterday jan we flew colombo sri lanka to par...</td>\n",
       "      <td>[yesterday, jan, we, flew, colombo, sri, lanka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First medium-haul experience with Air France....</td>\n",
       "      <td>first medium haul experience with air france t...</td>\n",
       "      <td>[first, medium, haul, experience, with, air, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOD-CDG-ATL: Was having some major issue with...</td>\n",
       "      <td>bod cdg atl was having some major issue with d...</td>\n",
       "      <td>[bod, cdg, atl, was, having, some, major, issu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  \\\n",
       "0  My flight from Dulles to Paris was great. My f...   \n",
       "1   I went for 2 months to Nice, when I arrived i...   \n",
       "2  Yesterday Jan 25, we flew Colombo, Sri Lanka t...   \n",
       "3   First medium-haul experience with Air France....   \n",
       "4   BOD-CDG-ATL: Was having some major issue with...   \n",
       "\n",
       "                                  remove_lower_punct  \\\n",
       "0  my flight from dulles to paris was great my fl...   \n",
       "1  i went for months to nice when i arrived in ni...   \n",
       "2  yesterday jan we flew colombo sri lanka to par...   \n",
       "3  first medium haul experience with air france t...   \n",
       "4  bod cdg atl was having some major issue with d...   \n",
       "\n",
       "                                            tokenise  \n",
       "0  [my, flight, from, dulles, to, paris, was, gre...  \n",
       "1  [i, went, for, months, to, nice, when, i, arri...  \n",
       "2  [yesterday, jan, we, flew, colombo, sri, lanka...  \n",
       "3  [first, medium, haul, experience, with, air, f...  \n",
       "4  [bod, cdg, atl, was, having, some, major, issu...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenise'] = data.apply(lambda row: nltk.word_tokenize(str(row[1])), axis=1)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d54e74a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d54e74a",
    "outputId": "2d1e9ca7-6f70-47da-e52e-c6764f847ce9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874b0150",
   "metadata": {
    "id": "874b0150"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e079954",
   "metadata": {
    "id": "8e079954"
   },
   "outputs": [],
   "source": [
    "data['remove_stopwords'] = data['tokenise'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e053b1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "4e053b1f",
    "outputId": "3a953d9c-2f53-41e9-d820-4d5118fc3d72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>remove_lower_punct</th>\n",
       "      <th>tokenise</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My flight from Dulles to Paris was great. My f...</td>\n",
       "      <td>my flight from dulles to paris was great my fl...</td>\n",
       "      <td>[my, flight, from, dulles, to, paris, was, gre...</td>\n",
       "      <td>[flight, dulles, paris, great, flight, attenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went for 2 months to Nice, when I arrived i...</td>\n",
       "      <td>i went for months to nice when i arrived in ni...</td>\n",
       "      <td>[i, went, for, months, to, nice, when, i, arri...</td>\n",
       "      <td>[went, months, nice, arrived, nice, baggage, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday Jan 25, we flew Colombo, Sri Lanka t...</td>\n",
       "      <td>yesterday jan we flew colombo sri lanka to par...</td>\n",
       "      <td>[yesterday, jan, we, flew, colombo, sri, lanka...</td>\n",
       "      <td>[yesterday, jan, flew, colombo, sri, lanka, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First medium-haul experience with Air France....</td>\n",
       "      <td>first medium haul experience with air france t...</td>\n",
       "      <td>[first, medium, haul, experience, with, air, f...</td>\n",
       "      <td>[first, medium, haul, experience, air, france,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOD-CDG-ATL: Was having some major issue with...</td>\n",
       "      <td>bod cdg atl was having some major issue with d...</td>\n",
       "      <td>[bod, cdg, atl, was, having, some, major, issu...</td>\n",
       "      <td>[bod, cdg, atl, major, issue, delta, airlines,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  \\\n",
       "0  My flight from Dulles to Paris was great. My f...   \n",
       "1   I went for 2 months to Nice, when I arrived i...   \n",
       "2  Yesterday Jan 25, we flew Colombo, Sri Lanka t...   \n",
       "3   First medium-haul experience with Air France....   \n",
       "4   BOD-CDG-ATL: Was having some major issue with...   \n",
       "\n",
       "                                  remove_lower_punct  \\\n",
       "0  my flight from dulles to paris was great my fl...   \n",
       "1  i went for months to nice when i arrived in ni...   \n",
       "2  yesterday jan we flew colombo sri lanka to par...   \n",
       "3  first medium haul experience with air france t...   \n",
       "4  bod cdg atl was having some major issue with d...   \n",
       "\n",
       "                                            tokenise  \\\n",
       "0  [my, flight, from, dulles, to, paris, was, gre...   \n",
       "1  [i, went, for, months, to, nice, when, i, arri...   \n",
       "2  [yesterday, jan, we, flew, colombo, sri, lanka...   \n",
       "3  [first, medium, haul, experience, with, air, f...   \n",
       "4  [bod, cdg, atl, was, having, some, major, issu...   \n",
       "\n",
       "                                    remove_stopwords  \n",
       "0  [flight, dulles, paris, great, flight, attenda...  \n",
       "1  [went, months, nice, arrived, nice, baggage, c...  \n",
       "2  [yesterday, jan, flew, colombo, sri, lanka, pa...  \n",
       "3  [first, medium, haul, experience, air, france,...  \n",
       "4  [bod, cdg, atl, major, issue, delta, airlines,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f851b31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f851b31",
    "outputId": "a7a025ee-3d94-4412-e7ab-27668a132e21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db70833e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "db70833e",
    "outputId": "94a4b1c8-0587-420e-9f6c-090025fc847c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>remove_lower_punct</th>\n",
       "      <th>tokenise</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>lemmatise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My flight from Dulles to Paris was great. My f...</td>\n",
       "      <td>my flight from dulles to paris was great my fl...</td>\n",
       "      <td>[my, flight, from, dulles, to, paris, was, gre...</td>\n",
       "      <td>[flight, dulles, paris, great, flight, attenda...</td>\n",
       "      <td>[flight, dulles, paris, great, flight, attenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went for 2 months to Nice, when I arrived i...</td>\n",
       "      <td>i went for months to nice when i arrived in ni...</td>\n",
       "      <td>[i, went, for, months, to, nice, when, i, arri...</td>\n",
       "      <td>[went, months, nice, arrived, nice, baggage, c...</td>\n",
       "      <td>[went, month, nice, arrived, nice, baggage, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday Jan 25, we flew Colombo, Sri Lanka t...</td>\n",
       "      <td>yesterday jan we flew colombo sri lanka to par...</td>\n",
       "      <td>[yesterday, jan, we, flew, colombo, sri, lanka...</td>\n",
       "      <td>[yesterday, jan, flew, colombo, sri, lanka, pa...</td>\n",
       "      <td>[yesterday, jan, flew, colombo, sri, lanka, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First medium-haul experience with Air France....</td>\n",
       "      <td>first medium haul experience with air france t...</td>\n",
       "      <td>[first, medium, haul, experience, with, air, f...</td>\n",
       "      <td>[first, medium, haul, experience, air, france,...</td>\n",
       "      <td>[first, medium, haul, experience, air, france,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOD-CDG-ATL: Was having some major issue with...</td>\n",
       "      <td>bod cdg atl was having some major issue with d...</td>\n",
       "      <td>[bod, cdg, atl, was, having, some, major, issu...</td>\n",
       "      <td>[bod, cdg, atl, major, issue, delta, airlines,...</td>\n",
       "      <td>[bod, cdg, atl, major, issue, delta, airline, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  \\\n",
       "0  My flight from Dulles to Paris was great. My f...   \n",
       "1   I went for 2 months to Nice, when I arrived i...   \n",
       "2  Yesterday Jan 25, we flew Colombo, Sri Lanka t...   \n",
       "3   First medium-haul experience with Air France....   \n",
       "4   BOD-CDG-ATL: Was having some major issue with...   \n",
       "\n",
       "                                  remove_lower_punct  \\\n",
       "0  my flight from dulles to paris was great my fl...   \n",
       "1  i went for months to nice when i arrived in ni...   \n",
       "2  yesterday jan we flew colombo sri lanka to par...   \n",
       "3  first medium haul experience with air france t...   \n",
       "4  bod cdg atl was having some major issue with d...   \n",
       "\n",
       "                                            tokenise  \\\n",
       "0  [my, flight, from, dulles, to, paris, was, gre...   \n",
       "1  [i, went, for, months, to, nice, when, i, arri...   \n",
       "2  [yesterday, jan, we, flew, colombo, sri, lanka...   \n",
       "3  [first, medium, haul, experience, with, air, f...   \n",
       "4  [bod, cdg, atl, was, having, some, major, issu...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [flight, dulles, paris, great, flight, attenda...   \n",
       "1  [went, months, nice, arrived, nice, baggage, c...   \n",
       "2  [yesterday, jan, flew, colombo, sri, lanka, pa...   \n",
       "3  [first, medium, haul, experience, air, france,...   \n",
       "4  [bod, cdg, atl, major, issue, delta, airlines,...   \n",
       "\n",
       "                                           lemmatise  \n",
       "0  [flight, dulles, paris, great, flight, attenda...  \n",
       "1  [went, month, nice, arrived, nice, baggage, ca...  \n",
       "2  [yesterday, jan, flew, colombo, sri, lanka, pa...  \n",
       "3  [first, medium, haul, experience, air, france,...  \n",
       "4  [bod, cdg, atl, major, issue, delta, airline, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatise words\n",
    "\n",
    "data['lemmatise'] = data['remove_stopwords'].apply(lambda x: [wordnet_lemmatizer.lemmatize(y) for y in x if y.isalpha()]) \n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ETy5INBV98H3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETy5INBV98H3",
    "outputId": "000ae00d-84a2-4a75-c62f-1a17f517fb82"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "corpus=[]\n",
    "for index, row in data.iterrows():\n",
    "    corpus.append(\", \".join(row[24]))\n",
    "X = tf_vectorizer.fit_transform(corpus)\n",
    "tf_vectorizer.get_feature_names_out()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09ecd4d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09ecd4d6",
    "outputId": "faf0b492-3ab6-482b-8dd9-6515120d9ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 227272)\t1\n",
      "  (0, 178635)\t1\n",
      "  (0, 410977)\t1\n",
      "  (0, 262168)\t1\n",
      "  (0, 226401)\t1\n",
      "  (0, 47577)\t1\n",
      "  (0, 553491)\t1\n",
      "  (0, 383165)\t1\n",
      "  (0, 243851)\t1\n",
      "  (0, 603087)\t1\n",
      "  (0, 88026)\t1\n",
      "  (0, 143437)\t1\n",
      "  (0, 535938)\t1\n",
      "  (0, 242441)\t1\n",
      "  (0, 332234)\t1\n",
      "  (0, 537444)\t1\n",
      "  (0, 317169)\t1\n",
      "  (0, 498935)\t1\n",
      "  (0, 425494)\t1\n",
      "  (0, 395516)\t1\n",
      "  (0, 341225)\t1\n",
      "  (0, 372272)\t1\n",
      "  (0, 620412)\t1\n",
      "  (0, 235597)\t1\n",
      "  (0, 259439)\t1\n",
      "  :\t:\n",
      "  (34390, 387044)\t1\n",
      "  (34391, 258953)\t1\n",
      "  (34391, 243802)\t1\n",
      "  (34391, 539651)\t1\n",
      "  (34391, 333217)\t1\n",
      "  (34391, 286694)\t1\n",
      "  (34391, 497644)\t1\n",
      "  (34391, 86580)\t1\n",
      "  (34391, 397342)\t1\n",
      "  (34391, 539418)\t1\n",
      "  (34391, 67285)\t1\n",
      "  (34391, 191263)\t1\n",
      "  (34391, 235635)\t1\n",
      "  (34391, 282253)\t1\n",
      "  (34391, 313380)\t1\n",
      "  (34391, 191261)\t1\n",
      "  (34391, 27951)\t1\n",
      "  (34391, 640008)\t1\n",
      "  (34391, 354932)\t1\n",
      "  (34391, 117445)\t1\n",
      "  (34391, 311924)\t1\n",
      "  (34391, 640015)\t1\n",
      "  (34391, 200351)\t1\n",
      "  (34391, 52080)\t1\n",
      "  (34391, 317179)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(34392, 643873)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "                            \n",
    "# join the processed data to be vectorised\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    vectors.append(\", \".join(row[4]))\n",
    "\n",
    "vectorised = vectorizer.fit_transform(vectors)\n",
    "\n",
    "print(vectorised)\n",
    "vectorised.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f44c8161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f44c8161",
    "outputId": "ab27f6e1-481e-4f59-d954-14361146e9f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>remove_lower_punct</th>\n",
       "      <th>tokenise</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>lemmatise</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My flight from Dulles to Paris was great. My f...</td>\n",
       "      <td>my flight from dulles to paris was great my fl...</td>\n",
       "      <td>[my, flight, from, dulles, to, paris, was, gre...</td>\n",
       "      <td>[flight, dulles, paris, great, flight, attenda...</td>\n",
       "      <td>[flight, dulles, paris, great, flight, attenda...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went for 2 months to Nice, when I arrived i...</td>\n",
       "      <td>i went for months to nice when i arrived in ni...</td>\n",
       "      <td>[i, went, for, months, to, nice, when, i, arri...</td>\n",
       "      <td>[went, months, nice, arrived, nice, baggage, c...</td>\n",
       "      <td>[went, month, nice, arrived, nice, baggage, ca...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday Jan 25, we flew Colombo, Sri Lanka t...</td>\n",
       "      <td>yesterday jan we flew colombo sri lanka to par...</td>\n",
       "      <td>[yesterday, jan, we, flew, colombo, sri, lanka...</td>\n",
       "      <td>[yesterday, jan, flew, colombo, sri, lanka, pa...</td>\n",
       "      <td>[yesterday, jan, flew, colombo, sri, lanka, pa...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First medium-haul experience with Air France....</td>\n",
       "      <td>first medium haul experience with air france t...</td>\n",
       "      <td>[first, medium, haul, experience, with, air, f...</td>\n",
       "      <td>[first, medium, haul, experience, air, france,...</td>\n",
       "      <td>[first, medium, haul, experience, air, france,...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOD-CDG-ATL: Was having some major issue with...</td>\n",
       "      <td>bod cdg atl was having some major issue with d...</td>\n",
       "      <td>[bod, cdg, atl, was, having, some, major, issu...</td>\n",
       "      <td>[bod, cdg, atl, major, issue, delta, airlines,...</td>\n",
       "      <td>[bod, cdg, atl, major, issue, delta, airline, ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>My first flight was cancelled last minute, ca...</td>\n",
       "      <td>my first flight was cancelled last minute caus...</td>\n",
       "      <td>[my, first, flight, was, cancelled, last, minu...</td>\n",
       "      <td>[first, flight, cancelled, last, minute, causi...</td>\n",
       "      <td>[first, flight, cancelled, last, minute, causi...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I booked with KLM which was a nightmare. I was...</td>\n",
       "      <td>i booked with klm which was a nightmare i was ...</td>\n",
       "      <td>[i, booked, with, klm, which, was, a, nightmar...</td>\n",
       "      <td>[booked, klm, nightmare, supposed, fly, air, f...</td>\n",
       "      <td>[booked, klm, nightmare, supposed, fly, air, f...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I booked my trip 2 months in advance. This ai...</td>\n",
       "      <td>i booked my trip months in advance this airlin...</td>\n",
       "      <td>[i, booked, my, trip, months, in, advance, thi...</td>\n",
       "      <td>[booked, trip, months, advance, airline, cance...</td>\n",
       "      <td>[booked, trip, month, advance, airline, cancel...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Frankfurt - Toronto via Paris CDG. My first l...</td>\n",
       "      <td>frankfurt toronto via paris cdg my first long ...</td>\n",
       "      <td>[frankfurt, toronto, via, paris, cdg, my, firs...</td>\n",
       "      <td>[frankfurt, toronto, via, paris, cdg, first, l...</td>\n",
       "      <td>[frankfurt, toronto, via, paris, cdg, first, l...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Frankfurt - Toronto via Paris CDG. My first lo...</td>\n",
       "      <td>frankfurt toronto via paris cdg my first long ...</td>\n",
       "      <td>[frankfurt, toronto, via, paris, cdg, my, firs...</td>\n",
       "      <td>[frankfurt, toronto, via, paris, cdg, first, l...</td>\n",
       "      <td>[frankfurt, toronto, via, paris, cdg, first, l...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  \\\n",
       "0  My flight from Dulles to Paris was great. My f...   \n",
       "1   I went for 2 months to Nice, when I arrived i...   \n",
       "2  Yesterday Jan 25, we flew Colombo, Sri Lanka t...   \n",
       "3   First medium-haul experience with Air France....   \n",
       "4   BOD-CDG-ATL: Was having some major issue with...   \n",
       "5   My first flight was cancelled last minute, ca...   \n",
       "6  I booked with KLM which was a nightmare. I was...   \n",
       "7   I booked my trip 2 months in advance. This ai...   \n",
       "8   Frankfurt - Toronto via Paris CDG. My first l...   \n",
       "9  Frankfurt - Toronto via Paris CDG. My first lo...   \n",
       "\n",
       "                                  remove_lower_punct  \\\n",
       "0  my flight from dulles to paris was great my fl...   \n",
       "1  i went for months to nice when i arrived in ni...   \n",
       "2  yesterday jan we flew colombo sri lanka to par...   \n",
       "3  first medium haul experience with air france t...   \n",
       "4  bod cdg atl was having some major issue with d...   \n",
       "5  my first flight was cancelled last minute caus...   \n",
       "6  i booked with klm which was a nightmare i was ...   \n",
       "7  i booked my trip months in advance this airlin...   \n",
       "8  frankfurt toronto via paris cdg my first long ...   \n",
       "9  frankfurt toronto via paris cdg my first long ...   \n",
       "\n",
       "                                            tokenise  \\\n",
       "0  [my, flight, from, dulles, to, paris, was, gre...   \n",
       "1  [i, went, for, months, to, nice, when, i, arri...   \n",
       "2  [yesterday, jan, we, flew, colombo, sri, lanka...   \n",
       "3  [first, medium, haul, experience, with, air, f...   \n",
       "4  [bod, cdg, atl, was, having, some, major, issu...   \n",
       "5  [my, first, flight, was, cancelled, last, minu...   \n",
       "6  [i, booked, with, klm, which, was, a, nightmar...   \n",
       "7  [i, booked, my, trip, months, in, advance, thi...   \n",
       "8  [frankfurt, toronto, via, paris, cdg, my, firs...   \n",
       "9  [frankfurt, toronto, via, paris, cdg, my, firs...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [flight, dulles, paris, great, flight, attenda...   \n",
       "1  [went, months, nice, arrived, nice, baggage, c...   \n",
       "2  [yesterday, jan, flew, colombo, sri, lanka, pa...   \n",
       "3  [first, medium, haul, experience, air, france,...   \n",
       "4  [bod, cdg, atl, major, issue, delta, airlines,...   \n",
       "5  [first, flight, cancelled, last, minute, causi...   \n",
       "6  [booked, klm, nightmare, supposed, fly, air, f...   \n",
       "7  [booked, trip, months, advance, airline, cance...   \n",
       "8  [frankfurt, toronto, via, paris, cdg, first, l...   \n",
       "9  [frankfurt, toronto, via, paris, cdg, first, l...   \n",
       "\n",
       "                                           lemmatise  Topic1  Topic2  Topic3  \\\n",
       "0  [flight, dulles, paris, great, flight, attenda...    0.01    0.01    0.01   \n",
       "1  [went, month, nice, arrived, nice, baggage, ca...    0.01    0.01    0.96   \n",
       "2  [yesterday, jan, flew, colombo, sri, lanka, pa...    0.00    0.00    0.98   \n",
       "3  [first, medium, haul, experience, air, france,...    0.00    1.00    0.00   \n",
       "4  [bod, cdg, atl, major, issue, delta, airline, ...    0.00    0.99    0.00   \n",
       "5  [first, flight, cancelled, last, minute, causi...    0.99    0.00    0.00   \n",
       "6  [booked, klm, nightmare, supposed, fly, air, f...    0.99    0.00    0.00   \n",
       "7  [booked, trip, month, advance, airline, cancel...    0.00    0.99    0.00   \n",
       "8  [frankfurt, toronto, via, paris, cdg, first, l...    0.00    0.00    0.99   \n",
       "9  [frankfurt, toronto, via, paris, cdg, first, l...    0.00    0.00    0.99   \n",
       "\n",
       "   Topic4  Topic5  Dominant_topic  \n",
       "0    0.98    0.01               4  \n",
       "1    0.01    0.01               3  \n",
       "2    0.00    0.00               3  \n",
       "3    0.00    0.00               2  \n",
       "4    0.00    0.00               2  \n",
       "5    0.00    0.00               1  \n",
       "6    0.00    0.00               1  \n",
       "7    0.00    0.00               2  \n",
       "8    0.00    0.00               3  \n",
       "9    0.00    0.00               3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initisalise LDA Model\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 5, # number of topics\n",
    "                                  random_state = 10,          # random state\n",
    "                                  evaluate_every = -1,      # compute perplexity every n iters, default: Don't\n",
    "                                  n_jobs = -1,              # Use all available CPUs\n",
    "                                 )\n",
    "\n",
    "lda_output = lda_model.fit_transform(vectorised)\n",
    "\n",
    "# column names\n",
    "\n",
    "topic_names = [\"Topic\" + str(i) for i in range(1, lda_model.n_components + 1)]\n",
    "\n",
    "# make the pandas dataframe\n",
    "\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns = topic_names)\n",
    "\n",
    "# get dominant topic for each document\n",
    "\n",
    "dominant_topic = (np.argmax(df_document_topic.values, axis=1)+1)\n",
    "df_document_topic['Dominant_topic'] = dominant_topic\n",
    "\n",
    "# join to original dataframes\n",
    "\n",
    "data = pd.merge(data, df_document_topic, left_index = True, right_index = True, how = 'outer')\n",
    "data['Dominant_topic']=data['Dominant_topic'].astype(int)\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27a1de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "0f27a1de",
    "outputId": "e52065db-46b8-420c-f21d-a794c0153385"
   },
   "outputs": [],
   "source": [
    "''''# initisalise LDA Model on TF IDF\n",
    "\n",
    "lda_model2 = LatentDirichletAllocation(n_components = 5, # number of topics\n",
    "                                  random_state = 10,          # random state\n",
    "                                  evaluate_every = -1,      # compute perplexity every n iters, default: Don't\n",
    "                                  n_jobs = -1,              # Use all available CPUs\n",
    "                                 )\n",
    "\n",
    "lda_output2 = lda_model2.fit_transform(X)\n",
    "\n",
    "# column names\n",
    "\n",
    "topic_names2 = [\"Topic_Tf_\" + str(i) for i in range(1, lda_model.n_components + 1)]\n",
    "\n",
    "# make the pandas dataframe\n",
    "\n",
    "df_document_topic2 = pd.DataFrame(np.round(lda_output2, 2), columns = topic_names2)\n",
    "\n",
    "# get dominant topic for each document\n",
    "\n",
    "dominant_topic_tf = (np.argmax(df_document_topic2.values, axis=1)+1)\n",
    "df_document_topic2['Dominant_topic_tf'] = dominant_topic_tf\n",
    "\n",
    "# join to original dataframes\n",
    "\n",
    "data = pd.merge(data, df_document_topic2, left_index = True, right_index = True, how = 'outer')\n",
    "display(data.head(10))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8TFD4gOZet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "0b8TFD4gOZet",
    "outputId": "5da99f54-330d-4a0c-e31a-3d9b8176c120"
   },
   "outputs": [],
   "source": [
    "# apply sentiment analysis\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_score_list = []\n",
    "sentiment_label_list = []\n",
    "\n",
    "for i in data['remove_lower_punct'].values.tolist():\n",
    "    sentiment_score = analyser.polarity_scores(i)\n",
    "\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Positive')\n",
    "    elif sentiment_score['compound'] > -0.05 and sentiment_score['compound'] < 0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Neutral')\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Negative')\n",
    "    \n",
    "data['sentiment'] = sentiment_label_list\n",
    "data['sentiment score'] = sentiment_score_list\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "K0yIArduWI02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "id": "K0yIArduWI02",
    "outputId": "c71f1281-08cf-416f-c9e6-4c02227fff22"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>relevance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa aa</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>1.200039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa absolutely</th>\n",
       "      <td>Topic2</td>\n",
       "      <td>1.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa account</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>1.199999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa actually</th>\n",
       "      <td>Topic5</td>\n",
       "      <td>1.199998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa adding</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>1.199995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>每位空乘人员见到旅客都会非常热情的说 您好</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>1.199999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>登机时 每位空乘人员见到旅客都会非常热情的说</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>1.199999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>空乘人员并没有因为疫情的原因对我们敬而远之 反而以更加热情的态度对待我们</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>1.199999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>第一入境地为西安 在这个疫情严重的特殊时期</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>1.199999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>飞行途中 空乘人员并没有因为疫情的原因对我们敬而远之</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>1.199999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>643873 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       topic  relevance_score\n",
       "aa aa                                 Topic1         1.200039\n",
       "aa absolutely                         Topic2         1.199997\n",
       "aa account                            Topic1         1.199999\n",
       "aa actually                           Topic5         1.199998\n",
       "aa adding                             Topic3         1.199995\n",
       "...                                      ...              ...\n",
       "每位空乘人员见到旅客都会非常热情的说 您好                 Topic3         1.199999\n",
       "登机时 每位空乘人员见到旅客都会非常热情的说                Topic3         1.199999\n",
       "空乘人员并没有因为疫情的原因对我们敬而远之 反而以更加热情的态度对待我们  Topic3         1.199999\n",
       "第一入境地为西安 在这个疫情严重的特殊时期                 Topic3         1.199999\n",
       "飞行途中 空乘人员并没有因为疫情的原因对我们敬而远之            Topic3         1.199999\n",
       "\n",
       "[643873 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topic_names\n",
    "\n",
    "df_topic_no = pd.DataFrame(df_topic_keywords.idxmax())\n",
    "df_scores = pd.DataFrame(df_topic_keywords.max())\n",
    "\n",
    "tmp = pd.merge(df_topic_no, df_scores, left_index=True, right_index=True)\n",
    "tmp.columns = ['topic', 'relevance_score']\n",
    "\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "SC_hoVImXH03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "SC_hoVImXH03",
    "outputId": "115ff0de-318c-4054-910b-7b64fd492937"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_topic</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>[premium economy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic2</td>\n",
       "      <td>[seat front]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic5</td>\n",
       "      <td>[business class]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>[cabin crew]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic4</td>\n",
       "      <td>[customer service]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dominant_topic          topic_name\n",
       "0         Topic1   [premium economy]\n",
       "1         Topic2        [seat front]\n",
       "2         Topic5    [business class]\n",
       "3         Topic3        [cabin crew]\n",
       "4         Topic4  [customer service]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_topics = []\n",
    "\n",
    "for i in tmp['topic'].unique():    \n",
    "    tmp_1 = tmp.loc[tmp['topic'] == i].reset_index()\n",
    "    tmp_1 = tmp_1.sort_values('relevance_score', ascending=False).head(1)\n",
    "\n",
    "    tmp_2 = []\n",
    "    tmp_2.append(tmp_1['topic'].unique()[0])\n",
    "    tmp_2.append(list(tmp_1['index'].unique()))\n",
    "    all_topics.append(tmp_2)\n",
    "\n",
    "all_topics = pd.DataFrame(all_topics, columns=['Dominant_topic', 'topic_name'])\n",
    "display(all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_uit5jFdu_0O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "_uit5jFdu_0O",
    "outputId": "3f30068e-9418-4642-967f-fd4e85d51958"
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.Chart(data).mark_bar().encode(\n",
    "    x='Dominant_topic:N',\n",
    "    y='count():Q',\n",
    "    color=\"sentiment:N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GJT9GRaS2g0u",
   "metadata": {
    "id": "GJT9GRaS2g0u"
   },
   "outputs": [],
   "source": [
    "!pip install pyldavis"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hackathon v01as.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
